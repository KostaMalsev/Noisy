<!doctype html>
<html>

<head>

  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

</head>



<body>

  <script>
  
  
    let audioCtx;
    let analyserNode;
    let bufferLength;
    let dataArray;
    let audioSourceNode;
    let processorNode;
    let stream;
    
    let biquadFilter;
    let convolver;
    let gainNode;

    
    //Peak detection control:
    let windowWidth = 22;
    let threshold = 130;
    
    let Q = 1; //bandwidth[hz]
    
    //The analyser freq sample window:
    let fftSize = 2048;
    
    //Biquad filter notch Freq
    let notchFreq = 50;
    //(a) Î” f = f s N = 10 , 000 1024 = 9.776 Hz .
    let bps = 16; //bps
    let channels = 2;


    
    let noiseFreqBin = [];//Get the noise freq bin


    let sample_rate;//audio sample rate
    

    function detectPeaks(data, windowWidth, threshold) {
      
      const peaks = [];
      
      for (let i = 0; i < data.length; i++) {
        
        const start = Math.max(0, i - windowWidth);
        const end = Math.min(data.length, i + windowWidth);
        let deltaAcc = 0;
        for (let a = start; a < end; a++) {
          deltaAcc += Math.abs(data[a - 1] - data[a]);
        }
        if (deltaAcc > threshold) {
          peaks.push(i);
        }
      }
      return peaks;
    }

    //Convert from frequency bin to hertz:
    function binToHerzt(binId,sampleFreq,fftSize)
    {
      return (binId * sampleFreq/fftSize);
      //0 * 44100 / 1024
      //1 * 44100 / 1024 =    43.1 Hz
    }

    //Function to initiate the audio process node:
    async function initAudioProcess() {


      try {

        console.log("Loading processor node...");

        await audioCtx.audioWorklet.addModule('processor.js');

        processorNode = new AudioWorkletNode(audioCtx, "denoise-processor");

        //Set the parameters:
        sample_rate = processorNode.parameters.get('sample_rate');

        sample_rate.setValueAtTime(audioCtx.sampleRate, audioCtx.currentTime);



        //Post message to encoder (Worker):
        processorNode.port.postMessage({
          cmd: 'init',
          config: {
            samplerate: audioCtx.sampleRate,
            bps: bps,
            channels: channels,
            //compression: 5
          }
        });

        //Proccess message from encoder:
        processorNode.port.onmessage = function(e) {

          console.log('Got message from noise processor:', e);

          if (e.data.cmd == 'end') {
            console.log('processor: finished working');
          }
        }
      } catch (e) {

        console.log('Error creating Audioworklet', e);

      }

    }



      //Function creates process pipline for audio stream:
        async function createPipeline()
        {
          
          //Connecting the lego:
          //audioSourceNode.connect(processorNode);
          
          //Set up audio node network
          audioSourceNode.connect(analyserNode);
          
          //audioSourceNode.connect(audioCtx.destination);//TBD@@
          
          analyserNode.connect(biquadFilter);
          
          biquadFilter.connect(audioCtx.destination);
          
          //biquadFilter.connect(convolver);
          
          //convolver.connect(gainNode);
          
          //gainNode.connect(audioCtx.destination);
          
          //Define the notch
          biquadFilter.type = "notch";
          biquadFilter.frequency.value = 1;//4900; //hz
          biquadFilter.gain.value = 300;//25;
          biquadFilter.Q.value = Q;//25;
          
          //analyserNode.connect(audioCtx.destination);

          
          //processorNode.connect(audio_context.destination);
            
        }







    //Get the mic  persmissions:
     const gotUserMedia = async (localMediaStream) => {

      console.log('success grabbing microphone');

      stream = localMediaStream;

      audioCtx = new AudioContext;

      audioSourceNode = audioCtx.createMediaStreamSource(stream);
      
      //Init the worklet:
      //await initAudioProcess();
      
      

      //var oscillator = audioCtx.createOscillator();
      //oscillator.type = 'sine';
      //oscillator.frequency.setValueAtTime(440, audioCtx.currentTime); // value in herrz
      //oscillator.start();

      //Create analyser node
      analyserNode = audioCtx.createAnalyser();
      analyserNode.fftSize = 2048; //1024;//256;
      

      bufferLength = analyserNode.frequencyBinCount;
      dataArray = new Float32Array(bufferLength);

      //Creating biquadFilter
       biquadFilter = audioCtx.createBiquadFilter();
       convolver = audioCtx.createConvolver();
       gainNode = audioCtx.createGain(); //@@TBD
       
       await createPipeline();


    }


    const userMediaFailed = function(e) {
      console.log(e);
    }


    // Get the user media:
    navigator.getUserMedia({
      video: false,
      //audio: true,
      audio: {
        autoGainControl: false,
        channelCount: 2,
        echoCancellation: false,
        noiseSuppression: false
      },

    }, gotUserMedia, userMediaFailed);

    


    //Create 2D canvas
    const canvas = document.createElement('canvas');
    canvas.style.position = 'absolute';
    canvas.style.top = 0;
    canvas.style.left = 0;
    canvas.width = window.innerWidth;
    canvas.height = window.innerHeight;
    document.body.appendChild(canvas);
    const canvasCtx = canvas.getContext('2d');
    canvasCtx.clearRect(0, 0, canvas.width, canvas.height);

    function draw() {
      
      //Schedule next redraw
      requestAnimationFrame(draw);

      if (typeof(dataArray) == 'undefined') return;

      //Get spectrum data
      analyserNode.getFloatFrequencyData(dataArray);


      //Detect peaks:
      let noiseFreqBin = detectPeaks(dataArray, windowWidth, threshold);
      
      

      noiseFreqBin.forEach(bin => {
        console.log('peak freq is',binToHerzt(bin, audioCtx.sampleRate, fftSize),'sample rate:',audioCtx.sampleRate);
        biquadFilter.frequency.value = binToHerzt(bin,audioCtx.sampleRate,fftSize); //hz
      });
      


      //Draw black background
      canvasCtx.fillStyle = 'rgb(0, 0, 0)';
      canvasCtx.fillRect(0, 0, canvas.width, canvas.height);



      //Draw spectrum:
      const barWidth = (canvas.width / bufferLength) * 2.5;
      let posX = 0;


      for (let i = 0; i < bufferLength; i++) {

        const barHeight = (dataArray[i] + 140) * 2;

        if (typeof(noiseFreqBin.find(freq => freq == i)) != 'undefined') {
          
          canvasCtx.fillStyle = 'green';
          //console.log('found peac')      
        } else {
          canvasCtx.fillStyle = 'rgb(' + Math.floor(barHeight + 100) + ', 50, 50)';
        }
        
        canvasCtx.fillRect(posX, canvas.height - barHeight / 2, barWidth, barHeight / 2);
        posX += barWidth + 1;
        
      }
    }

    draw();
    
    
    /*
    The first bin in the FFT is DC (0 Hz), the second bin is Fs / N, where Fs is the sample rate and N is the size of the FFT. The next bin is 2 * Fs / N. To express this in general terms, the nth bin is n * Fs / N.
    So if your sample rate, Fs is say 44.1 kHz and your FFT size, N is 1024, then the FFT output bins are at
    */
    
    
  </script>
</body>

</html>